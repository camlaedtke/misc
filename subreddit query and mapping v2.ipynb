{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Reddit Data\n",
    "\n",
    "The notebook demonstrates how to extract massive amounts of reddit data in order to build a map of subreddits. \n",
    "\n",
    "Data extracted from https://files.pushshift.io/reddit/comments/\n",
    "- Files are around 2-10 times larger when uncompressed\n",
    "- Uncompressed files must be renamed to have .json file extension\n",
    "\n",
    "Code adapted from https://github.com/lmcinnes/subreddit_mapping\n",
    "\n",
    "Credit to reddit user /u/Stuck_In_the_Matrix and /u/hoffa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hdbscan\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "import adjustText\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import scipy.sparse as ss\n",
    "from os.path import isfile\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value, CustomJS\n",
    "from bokeh.models.mappers import LinearColorMapper\n",
    "from bokeh.palettes import plasma\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# sns.set_context('poster')\n",
    "# sns.set_style('white')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SQL Database\n",
    "\n",
    "Since the dataset is huge (71,826,552 comments), we need to read and write to the SQL database in chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subset(filepath, start=0, end=10):\n",
    "    comments = []\n",
    "    with open(filepath) as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            if (idx >= start) and (idx < end):\n",
    "                comment = json.loads(line)\n",
    "                comments.append(comment)\n",
    "            elif idx >= end:\n",
    "                break                \n",
    "        return comments\n",
    "    \n",
    "    \n",
    "def write_to_database(db_conn, json_fp, chunk_size):\n",
    "    batch_no=1\n",
    "    for chunk in pd.read_json(json_fp, chunksize=chunk_size, lines=True):\n",
    "        try: \n",
    "            chunk.to_sql('reddit_comments', db_conn, if_exists='append')\n",
    "        except sqlalchemy.exc.SQLAlchemyError as e: \n",
    "            print(\"\\n  {}\".format(e.orig))\n",
    "        print('\\rindex: {}'.format(batch_no), end='')\n",
    "        batch_no+=1\n",
    "        \n",
    "        \n",
    "def create_database(database, json_fp, n_files, comments_per_file, chunk_size, columns_to_drop):\n",
    "    for idx in range(0, n_files):\n",
    "        print(\"########## File {} ##########\".format(idx+1))\n",
    "        start = int(idx*comments_per_file)\n",
    "        end = int(start + comments_per_file)\n",
    "        print(\"Extracting comments {} - {}\".format(start, end))\n",
    "        comments = extract_subset(filepath=json_fp, start=start, end=end)\n",
    "        df = pd.DataFrame(comments)\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        if \"media_metadata\" in df.columns:\n",
    "            df = df.drop(columns=[\"media_metadata\"])\n",
    "        df.to_json(\"data/RC_2016-10_chunk.json\", orient='records', lines=True)\n",
    "\n",
    "        print(\"Writing to database\")\n",
    "        write_to_database(\n",
    "            db_conn=database, \n",
    "            json_fp=\"data/RC_2016-10_chunk.json\", \n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_engine('sqlite:///reddit_database_sample.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_cols = [\"associated_award\", \"all_awardings\", \"author_flair_richtext\", \"awarders\", \"gildings\",\n",
    "             \"author_flair_background_color\", \"author_flair_css_class\", \"author_flair_template_id\", \n",
    "             \"author_flair_text_color\", \"author_flair_type\", \"author_patreon_flair\", \"author_premium\",\n",
    "             \"can_gild\", \"can_mod_post\", \"collapsed_because_crowd_control\", \"distinguished\", \n",
    "             \"locked\", \"no_follow\", \"subreddit_id\", \"subreddit_name_prefixed\"]\n",
    "\n",
    "create_database(\n",
    "    database=conn, \n",
    "    json_fp='data/RC_2019-12.json',\n",
    "    n_files=50, \n",
    "    comments_per_file=1000000, \n",
    "    chunk_size=100000,\n",
    "    columns_to_drop=drop_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql_query(\"\"\"SELECT * FROM reddit_comments\"\"\", conn)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"reddit_database_sample.db\"\n",
    "\n",
    "conn = sqlite3.connect(DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of number of users in each subreddit, and save the result a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY1 = \"\"\"\n",
    "CREATE TABLE subr_users AS\n",
    "    SELECT subreddit, authors, DENSE_RANK() OVER (ORDER BY authors DESC) AS rank_authors\n",
    "    FROM (SELECT subreddit, SUM(1) as authors\n",
    "         FROM (SELECT subreddit, author, COUNT(1) as cnt \n",
    "             FROM reddit_comments\n",
    "             WHERE author NOT LIKE '%bot'\n",
    "             GROUP BY subreddit, author HAVING cnt > 0)\n",
    "         GROUP BY subreddit) t\n",
    "    ORDER BY authors DESC;\n",
    "\"\"\"\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute(QUERY1)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql_query(\"\"\"SELECT * FROM subr_users\"\"\", conn)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new table, we create a list of number of users who authored at least 10 posts in pairs of subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY2 = \"\"\"\n",
    "CREATE TABLE overlapping_subr_users AS\n",
    "    SELECT t1.subreddit, t2.subreddit, SUM(1) AS NumOverlaps\n",
    "    FROM (SELECT subreddit, author, COUNT(1) AS cnt \n",
    "         FROM reddit_comments\n",
    "         WHERE author NOT LIKE '%bot'\n",
    "         AND subreddit IN (SELECT subreddit FROM subr_users\n",
    "           WHERE rank_authors>200 AND rank_authors<2201)\n",
    "         GROUP BY subreddit, author HAVING cnt > 10) t1\n",
    "    JOIN (SELECT subreddit, author, COUNT(1) as cnt \n",
    "         FROM reddit_comments\n",
    "         WHERE author NOT LIKE '%bot'\n",
    "         GROUP BY subreddit, author HAVING cnt > 10) t2\n",
    "    ON t1.author=t2.author\n",
    "    WHERE t1.subreddit!=t2.subreddit\n",
    "    GROUP BY t1.subreddit, t2.subreddit\n",
    "\"\"\"\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute(QUERY2)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the result of the second query as a dataframe. Edit the column names and store it for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM overlapping_subr_users\"\"\", conn)\n",
    "df = df.rename(columns={\"subreddit\":\"t1_subreddit\", \"subreddit:1\":\"t2_subreddit\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"t1_subreddit unique subreddits: {}\".format(len(df[\"t1_subreddit\"].unique())))\n",
    "print(\"t2_subreddit unique subreddits: {}\".format(len(df[\"t2_subreddit\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_t1 = df[\"t1_subreddit\"].unique().tolist()\n",
    "unique_t2 = df[\"t2_subreddit\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"t1_subreddit\"].isin(unique_t2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/subreddit_overlaps_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/subreddit_overlaps_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: rank the subreddits, take out the last few thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.sort_values(by=\"NumOverlaps\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = raw_data.loc[raw_data.NumOverlaps > 2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of pairwise commenter overlaps: {}\".format(len(raw_data)))\n",
    "print(\"t1_subreddit unique subreddits: {}\".format(len(raw_data[\"t1_subreddit\"].unique())))\n",
    "print(\"t2_subreddit unique subreddits: {}\".format(len(raw_data[\"t2_subreddit\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the subreddits so that they are indexed in order of popularity. Popularity is defined by the total number of unique commenters in each subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t1_subreddit if it is larger?\n",
    "subreddit_popularity = raw_data.groupby('t2_subreddit')['NumOverlaps'].sum()\n",
    "subreddits = np.array(subreddit_popularity.sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the data into a matrix such that rows and columns are both indexed by subreddits, and the entry at position (i,j) is the number of overlaps bwteen the ith and jth subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subreddit-to-integer-index map to convert the subreddit names in the table into numeric row and column indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = dict(np.vstack([subreddits, np.arange(subreddits.shape[0])]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in index_map.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = raw_data.NumOverlaps\n",
    "row_indices = raw_data.t2_subreddit.map(index_map)\n",
    "col_indices = raw_data.t1_subreddit.map(index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(row_indices[row_indices.isna()==True]))\n",
    "print(len(col_indices[col_indices.isna()==True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sparse matrix. This format requires us to specify triples of row, column, and value for each non-zero entry in the matrix. The COO matrix constructor accepts this as a triple of arrays: the first array is the values, the second and third are arrays of row and column indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = ss.coo_matrix((values, (row_indices,col_indices)),\n",
    "                              shape=(subreddits.shape[0], subreddits.shape[0]),\n",
    "                              dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_prob_matrix = count_matrix.tocsr()\n",
    "conditional_prob_matrix = normalize(conditional_prob_matrix, norm='l1', copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting subreddit vectors into a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors = TruncatedSVD(n_components=500, random_state=1).fit_transform(conditional_prob_matrix)\n",
    "reduced_vectors = normalize(reduced_vectors, norm='l2', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(verbose=1, n_components=2, perplexity=50)\n",
    "\n",
    "subreddit_map = tsne.fit_transform(reduced_vectors[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map_df = pd.DataFrame(subreddit_map, columns=('x', 'y'))\n",
    "subreddit_map_df['subreddit'] = subreddits[:10000]\n",
    "subreddit_map_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_samples=5, min_cluster_size=20).fit(subreddit_map)\n",
    "cluster_ids = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map_df['cluster'] = cluster_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "# To output to a static html file, comment out the previous\n",
    "# line, and uncomment the line below.\n",
    "# output_file('subreddit_interactive_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a color palette and map clusters to colors\n",
    "palette = ['#777777'] + list(plasma(cluster_ids.max()))\n",
    "\n",
    "colormap = LinearColorMapper(palette=palette, low=-1, high=cluster_ids.max())\n",
    "\n",
    "color_dict = {\n",
    "    'field': 'cluster', \n",
    "    'transform': colormap\n",
    "}\n",
    "\n",
    "# Set fill alpha globally\n",
    "subreddit_map_df['fill_alpha'] = np.exp((subreddit_map.min() - subreddit_map.max()) / 5.0) + 0.05\n",
    "\n",
    "# Build a column data source\n",
    "plot_data = ColumnDataSource(subreddit_map_df)\n",
    "\n",
    "# Custom callback for alpha adjustment\n",
    "jscode=\"\"\"\n",
    "    var data = source.data;\n",
    "    var start = cb_obj.start;\n",
    "    var end = cb_obj.end;\n",
    "    alpha = data['fill_alpha']\n",
    "    for (i = 0; i < alpha.length; i++) {\n",
    "         alpha[i] = Math.exp((start - end) / 5.0) + 0.05;\n",
    "    }\n",
    "    source.trigger('change');\n",
    "\"\"\"\n",
    "\n",
    "# Create the figure and add tools\n",
    "fig = figure(\n",
    "    title='A Map of Subreddits',\n",
    "    plot_width = 900,\n",
    "    plot_height = 900,\n",
    "    tools= ('pan, wheel_zoom, box_zoom,''box_select, reset'),\n",
    "    active_scroll=u'wheel_zoom'\n",
    ")\n",
    "\n",
    "# tools= ('pan, wheel_zoom, box_zoom,''box_select, resize, reset'),\n",
    "fig.add_tools(\n",
    "    HoverTool(\n",
    "        tooltips = OrderedDict(\n",
    "            [('subreddit', '@subreddit'), ('cluster', '@cluster')]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# draw the subreddits as circles on the plot\n",
    "fig.circle(\n",
    "    u'x', u'y', \n",
    "    source = plot_data,\n",
    "    fill_color = color_dict, \n",
    "    line_color = None, \n",
    "    fill_alpha = 'fill_alpha',\n",
    "    size = 10, \n",
    "    hover_line_color = u'black'\n",
    ")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=plot_data,\n",
    "    ), \n",
    "    code=jscode\n",
    ")\n",
    "\n",
    "# fig.x_range.callback = CustomJS(args=dict(source=plot_data), code=jscode)\n",
    "# fig.y_range.callback = CustomJS(args=dict(source=plot_data), code=jscode)\n",
    "\n",
    "# fig.x_range.js_on_change(\"plot_data\", callback)\n",
    "# fig.y_range.js_on_change(\"plot_data\", callback)\n",
    "\n",
    "# configure visual elements of the plot\n",
    "fig.title.text_font_size = value('18pt')\n",
    "fig.title.align = 'center'\n",
    "fig.xaxis.visible = False\n",
    "fig.yaxis.visible = False\n",
    "fig.grid.grid_line_color = None\n",
    "fig.outline_line_color = '#222222'\n",
    "\n",
    "# # display the figure\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_bounds(dataframe, subreddit):\n",
    "    # Find the cluster the subreddit belongs to\n",
    "    cluster = dataframe.cluster[\n",
    "        dataframe.subreddit == subreddit].values[0]\n",
    "    if cluster == -1:\n",
    "        warn('This subreddit was lost as noise and not in any cluster')\n",
    "        \n",
    "    # Extract the dubset of the dataframe that is the cluster\n",
    "    sub_dataframe = dataframe[dataframe.cluster == cluster]\n",
    "    \n",
    "    x_min = sub_dataframe.x.min()\n",
    "    x_max = sub_dataframe.x.max()\n",
    "    x_padding = (x_max - x_min) * 0.5\n",
    "    x_min -= x_padding\n",
    "    x_max += x_padding\n",
    "    \n",
    "    y_min = sub_dataframe.y.min()\n",
    "    y_max = sub_dataframe.y.max()\n",
    "    y_padding = (y_max - y_min) * 0.5\n",
    "    y_min -= y_padding\n",
    "    y_max += y_padding\n",
    "\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def data_in_bounds(dataframe, bounds):\n",
    "    return dataframe[\n",
    "        (dataframe.x > bounds[0]) &\n",
    "        (dataframe.x < bounds[1]) &\n",
    "        (dataframe.y > bounds[2]) &\n",
    "        (dataframe.y < bounds[3])\n",
    "    ]\n",
    "\n",
    "\n",
    "def plot_cluster(dataframe, subreddit):\n",
    "    # Build a color map to match the Bokeh plot\n",
    "    colormap = dict(zip(\n",
    "        np.unique(dataframe.cluster),\n",
    "        ['#777777'] + sns.color_palette('plasma', dataframe.cluster.max() + 1).as_hex()\n",
    "    ))\n",
    "    subregion_defined = True\n",
    "    \n",
    "    # Figure and gridspec to layout axes\n",
    "    fig = plt.figure(figsize=(16,10))\n",
    "    gs = GridSpec(3, 3)\n",
    "    \n",
    "    # First axes, spanning most of the figure\n",
    "    # Contains just the points in a region \n",
    "    # around the points in the cluster\n",
    "    ax1 = plt.subplot(gs[:,:2])\n",
    "    try:\n",
    "        bounds = cluster_bounds(dataframe, subreddit)\n",
    "    except IndexError:\n",
    "        ax1.text(0.5, 0.5, 'Subreddit {} not found!'.format(subreddit), \n",
    "                 horizontalalignment='center', verticalalignment='center',\n",
    "                 transform=ax1.transAxes, fontsize=18)\n",
    "        subregion_defined = False\n",
    "    \n",
    "    if subregion_defined:\n",
    "        to_plot = data_in_bounds(dataframe, bounds)\n",
    "        ax1.scatter(to_plot.x, to_plot.y, c=to_plot.cluster.map(colormap), s=30, alpha=0.5)\n",
    "    \n",
    "        # We want to add text labels. We subsample up to 50 labels\n",
    "        # And then use adjustText to get them non-overlapping\n",
    "        text_elements = []\n",
    "        for row in to_plot.sample(n=min(len(to_plot),50), random_state=0).values:\n",
    "            if row[2] != subreddit:\n",
    "                text_elements.append(ax1.text(row[0], row[1], row[2], alpha=0.5, fontsize=9))\n",
    "        row = to_plot[to_plot.subreddit == subreddit].values[0]\n",
    "        text_elements.append(ax1.text(row[0], row[1], row[2], \n",
    "                                      color='g',\n",
    "                                      alpha=0.5, fontsize=11))\n",
    "        adjustText.adjust_text(text_elements, ax=ax1, lim=100,\n",
    "                               force_text=0.1, force_points=0.1,\n",
    "                               arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.5))\n",
    "    \n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    ax1.yaxis.set_ticklabels([])\n",
    "\n",
    "    # Second axes, center right of the figure\n",
    "    # Plots all the data and a rectangle\n",
    "    # Showing the area selected out\n",
    "    ax2 = plt.subplot(gs[1,2])\n",
    "    ax2.scatter(dataframe.x, dataframe.y, s=20,\n",
    "                c=dataframe.cluster.map(colormap), alpha=0.05)\n",
    "    \n",
    "    if subregion_defined:\n",
    "        ax2.add_patch(Rectangle(xy=(bounds[0], bounds[2]),\n",
    "                                    width=(bounds[1] - bounds[0]),\n",
    "                                    height=(bounds[3] - bounds[2]),\n",
    "                                    edgecolor='k', facecolor='none', lw=1))\n",
    "    ax2.xaxis.set_ticklabels([])\n",
    "    ax2.yaxis.set_ticklabels([])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if subregion_defined:\n",
    "        # Now we make use of the power of matplotlib transforms\n",
    "        # to draw line from the subselected rectangle in axes2\n",
    "        # all the way to the bounds of axes1\n",
    "        trans_figure = fig.transFigure.inverted()\n",
    "\n",
    "        ax1_coord = trans_figure.transform(ax1.transAxes.transform((1,0)))\n",
    "        ax2_coord = trans_figure.transform(ax2.transData.transform((bounds[1],bounds[2])))\n",
    "        connector1 = Line2D((ax1_coord[0],ax2_coord[0]),(ax1_coord[1],ax2_coord[1]),\n",
    "                              transform=fig.transFigure, lw=1, color='k')\n",
    "        ax1_coord = trans_figure.transform(ax1.transAxes.transform((1,1)))\n",
    "        ax2_coord = trans_figure.transform(ax2.transData.transform((bounds[1],bounds[3])))\n",
    "        connector2 = Line2D((ax1_coord[0],ax2_coord[0]),(ax1_coord[1],ax2_coord[1]),\n",
    "                              transform=fig.transFigure, lw=1, color='k')\n",
    "\n",
    "        fig.lines = [connector1, connector2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(subreddit_map_df, 'trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(subreddit_map_df, 'MachineLearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual, fixed, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(plot_cluster, \n",
    "                dataframe=fixed(subreddit_map_df), \n",
    "                subreddit=Text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherent_clusters = np.argsort(clusterer.cluster_persistence_)[-10:][::-1]\n",
    "coherence = np.sort(clusterer.cluster_persistence_)[-10:][::-1]\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = plt.gca()\n",
    "ax.bar(np.arange(10), coherence)\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_xticklabels(coherent_clusters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_by_id(dataframe, cluster_id):\n",
    "    subreddits_in_cluster = np.array(dataframe.subreddit[cluster_ids == cluster_id])\n",
    "    plot_cluster(dataframe, subreddits_in_cluster[0])\n",
    "    plt.gcf().text(0.5, 0.98, 'Cluster {}'.format(cluster_id), ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_by_id(subreddit_map_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
